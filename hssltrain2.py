# -*- coding: utf-8 -*-
"""
Created on Wed Jul  9 20:06:27 2025

@author: wuli
"""

# -*- coding: utf-8 -*-
"""
hssl

@author: wuli
"""

# -*- coding: utf-8 -*-
"""
åŒæ•°æ®é›†PPOå®Œæ•´ç‰ˆ - æ¯500ä»£ä¿å­˜æœ‰æ•ˆç»„åˆ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
from lifelines.statistics import logrank_test
import os
import time
from datetime import datetime
from collections import deque
from torch.distributions import Bernoulli
from joblib import Parallel, delayed, parallel_backend  # ä½¿ç”¨joblibè¿›è¡Œå¹¶è¡Œè®¡ç®—

# ================== å‚æ•°é…ç½® ==================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ–‡ä»¶è·¯å¾„
DATA_PATHS = ["train_test.xlsx", "Ntrain_Ntest.xlsx"]
OUTPUT_DIR = "results_PPO"
FEATURE_CSV = os.path.join(OUTPUT_DIR, "selected_features.csv")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è®­ç»ƒå‚æ•°
EPISODES = 400000
BATCH_SIZE = 256
GAMMA = 0.99
LR = 1e-4
CLIP_EPS = 0.03
PPO_EPOCHS = 3
MINI_BATCH_SIZE = 128
ENTROPY_COEF = 0.1
HISTORY_WINDOW = 1000

# ç½‘ç»œç»“æ„å‚æ•°ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
FC1_DIM = 2048
FC2_DIM = 2048
FC3_DIM = 2048

# å¥–åŠ±å‚æ•°
MIN_FEATURES = 5
MAX_FEATURES = 25
FEATURE_PENALTY = 40
SAMPLE_PENALTY = 120
MIN_SAMPLES_PCT = 0.1
BASE_REWARD = 200
REWARD_SCALE = 100
INIT_BIAS = -6.8

# ================== æ•°æ®åŠ è½½ ==================
def load_data(paths):
    datasets = []
    feature_dim = None
    for path in paths:
        df = pd.read_excel(path)
        time_col = df.iloc[:, 0].values.astype(np.float32)
        event_col = df.iloc[:, 1].values
        features = df.iloc[:, 2:].values.astype(np.float32)
        
        if feature_dim is None:
            feature_dim = features.shape[1]
        else:
            assert feature_dim == features.shape[1], "ç‰¹å¾ç»´åº¦ä¸ä¸€è‡´ï¼"
        
        datasets.append((time_col, event_col, features))
    return datasets, feature_dim

# ================== ç½‘ç»œç»“æ„ ==================
class ActorCritic(nn.Module):
    def __init__(self, feature_dim):
        super().__init__()
        self.STATE_DIM = feature_dim + 3  # åŸºç¡€ç‰¹å¾ + ç»Ÿè®¡ç‰¹å¾
        self.ACTION_DIM = feature_dim
        
        self.shared_layers = nn.Sequential(
            nn.Linear(self.STATE_DIM, FC1_DIM),
            nn.LayerNorm(FC1_DIM),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(FC1_DIM, FC2_DIM),
            nn.LayerNorm(FC2_DIM),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(FC2_DIM, FC3_DIM),
            nn.LayerNorm(FC3_DIM),
            nn.ReLU(),
        )
        self.actor = nn.Linear(FC3_DIM, self.ACTION_DIM)
        self.critic = nn.Sequential(
            nn.Linear(FC3_DIM, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )
        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.shared_layers:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                nn.init.constant_(m.bias, 0.1)
        nn.init.normal_(self.actor.weight, std=0.01)
        nn.init.constant_(self.actor.bias, INIT_BIAS)
        nn.init.xavier_normal_(self.critic[-1].weight)

    def forward(self, x):
        x = self.shared_layers(x)
        action_logits = self.actor(x)
        action_probs = torch.sigmoid(action_logits / 1.5)
        state_value = self.critic(x).squeeze()
        return action_probs, state_value

# ================== PPO Agent ==================
class PPOAgent:
    def __init__(self, feature_dim):
        self.actor_critic = ActorCritic(feature_dim).to(device)
        self.optimizer = optim.AdamW(self.actor_critic.parameters(), lr=LR, weight_decay=1e-4)
        self.buffer = []
        self.recent_actions = deque(maxlen=100)

    def _add_state_features(self, base_state, action):
        selected_count = np.sum(action)
        return np.concatenate([
            base_state,
            [selected_count/MAX_FEATURES,
             max(0, MIN_FEATURES-selected_count)/MIN_FEATURES,
             max(0, selected_count-MAX_FEATURES)/self.actor_critic.ACTION_DIM]
        ])

    def act(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            action_probs, value = self.actor_critic(state_tensor)
        dist = Bernoulli(probs=action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum()
        return action.cpu().numpy().squeeze().astype(np.int64), log_prob.item(), value.item()

# ================== ç”Ÿå­˜åˆ†æå·¥å…· ==================
def safe_logrank(args):
    action, dataset = args
    time, event, features = dataset
    
    selected = (features @ action) > 0.5
    if np.sum(selected) < 3 or np.sum(~selected) < 3:
        return (1.0, False)
    
    try:
        result = logrank_test(time[selected], time[~selected], 
                            event_observed_A=event[selected], 
                            event_observed_B=event[~selected])
        return (result.p_value, True)
    except:
        return (1.0, False)

# ================== å¥–åŠ±è®¡ç®— ==================
def calculate_reward(action, datasets, agent, episode, bootstrap_samples):
    p_values = []
    valid_p = True
    num_features = np.sum(action)
    valid_features = MIN_FEATURES <= num_features <= MAX_FEATURES
    sample_penalties = 0
    
    # ä½¿ç”¨joblibå¹¶è¡Œæ‰§è¡ŒåŸå§‹logrankæ£€éªŒ
    args_list = [(action, dataset) for dataset in datasets]
    with parallel_backend('loky', n_jobs=-1):
        results = Parallel(verbose=0)(delayed(safe_logrank)(arg) for arg in args_list)
    
    # å¤„ç†åŸå§‹ç»“æœï¼ˆæ ¹æ®æ•°æ®é›†ç±»å‹ç»™äºˆä¸åŒå¥–åŠ±ï¼‰
    dataset_rewards = []
    for i, (p, sample_valid) in enumerate(results):
        p_values.append(p)
        if not sample_valid:
            sample_penalties += 1
        
        # è®­ç»ƒç»„è‚ºç™Œï¼ˆç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼‰éœ€è¦p<0.05
        if i == 0:
            if p < 0.05:
                reward = BASE_REWARD * np.sqrt(-np.log(max(p, 1e-20)))
            else:
                reward = -BASE_REWARD
        # Ntrainï¼ˆç¬¬äºŒä¸ªæ•°æ®é›†ï¼‰éœ€è¦p>=0.05
        else:
            if p >= 0.05:
                reward = BASE_REWARD * np.sqrt(-np.log(max(1-p, 1e-20)))
            else:
                reward = -BASE_REWARD
        
        dataset_rewards.append(reward)
    
    # Bootstrapå¥–åŠ±è®¡ç®—
    bootstrap_reward = 0
    bootstrap_passes = []
    for i, samples in enumerate(bootstrap_samples):
        args_list = [(action, sample) for sample in samples]
        with parallel_backend('loky', n_jobs=-1):
            bootstrap_results = Parallel(verbose=0)(delayed(safe_logrank)(arg) for arg in args_list)
        
        pass_count = 0
        for p, valid in bootstrap_results:
            if valid:
                if i == 0 and p < 0.05:
                    pass_count += 1
                elif i == 1 and p >= 0.05:
                    pass_count += 1
        bootstrap_passes.append(pass_count)
        bootstrap_reward += (pass_count / 100) * 150
    
    # ç‰¹å¾æƒ©ç½š
    feature_penalty = 0
    if num_features < MIN_FEATURES:
        gap = MIN_FEATURES - num_features
        feature_penalty += (gap ** 1.5) * FEATURE_PENALTY
    elif num_features > MAX_FEATURES:
        gap = num_features - MAX_FEATURES
        feature_penalty += (gap ** 1.5) * FEATURE_PENALTY
    
    # æ€»å¥–åŠ±è®¡ç®—ï¼ˆç§»é™¤äº†é‡å¤æƒ©ç½šéƒ¨åˆ†ï¼‰
    total_reward = (
        sum(dataset_rewards) 
        + bootstrap_reward
        - feature_penalty
        - sample_penalties * SAMPLE_PENALTY
    ) / REWARD_SCALE
    
    valid = valid_p and valid_features
    
    return (
        np.clip(total_reward, -100.0, 100.0),
        valid,
        p_values,
        num_features,
        bootstrap_passes
    )

# ================== è®­ç»ƒè¿›åº¦å›¾ä¿å­˜å‡½æ•° ==================
def save_training_progress(episode_logs, output_dir, episode):
    """æ¯500ä»£ä¿å­˜ä¸€æ¬¡è®­ç»ƒè¿›åº¦å›¾"""
    try:
        import matplotlib
        matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
        import matplotlib.pyplot as plt
        
        log_df = pd.DataFrame(episode_logs)
        
        plt.figure(figsize=(18, 6))
        
        # æ€»æŸå¤±å›¾
        plt.subplot(1, 3, 1)
        plt.plot(log_df['episode'], log_df['total_loss'], label='æ€»æŸå¤±')
        plt.plot(log_df['episode'], log_df['value_loss'], label='ä»·å€¼æŸå¤±')
        plt.xlabel('Episode')
        plt.ylabel('Loss')
        plt.legend()
        
        # å¥–åŠ±å›¾
        plt.subplot(1, 3, 2)
        plt.plot(log_df['episode'], log_df['reward'])
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title("å¥–åŠ±å˜åŒ–")
        
        # ç­–ç•¥ç†µå›¾
        plt.subplot(1, 3, 3)
        plt.plot(log_df['episode'], log_df['entropy'])
        plt.xlabel('Episode')
        plt.ylabel('Entropy')
        plt.title("ç­–ç•¥ç†µ")
        
        plt.tight_layout()
        plot_path = os.path.join(output_dir, f"training_progress_ep{episode}.png")
        plt.savefig(plot_path, dpi=100, bbox_inches='tight')  # é™ä½dpiå‡å°‘æ–‡ä»¶å¤§å°
        plt.close()
        
        return plot_path
    except Exception as e:
        print(f"ä¿å­˜è®­ç»ƒè¿›åº¦å›¾å¤±è´¥: {str(e)}")
        return None

# ================== è®­ç»ƒæµç¨‹ ==================
def train(datasets, feature_dim, bootstrap_samples):
    agent = PPOAgent(feature_dim)
    start_time = time.time()
    # +++ æ·»åŠ æ¨¡å‹åŠ è½½åŠŸèƒ½ +++
    MODEL_PATH = "final_model.pth"
    if os.path.exists(MODEL_PATH):
        agent.actor_critic.load_state_dict(torch.load(MODEL_PATH))
        print(f"jiazaiwanchengâœ… åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹æƒé‡: {MODEL_PATH}1111")
    # +++++++++++++++++++++++
    base_state = np.zeros(feature_dim, dtype=np.float32)
    state = agent._add_state_features(base_state, np.zeros(feature_dim))
    
    episode_logs = []
    recent_losses = []
    recent_value_losses = []
    recent_entropies = []
    valid_combinations_cache = []  # æœ‰æ•ˆç»„åˆç¼“å­˜

    for episode in range(EPISODES):
        action, log_prob, value = agent.act(state)
        reward, valid, p_values, num_features, bootstrap_passes = calculate_reward(
            action, datasets, agent, episode, bootstrap_samples
        )
        agent.recent_actions.append(action)
        
        # çŠ¶æ€è½¬ç§»
        next_base = 0.9 * base_state + 0.1 * action.astype(np.float32)
        next_state = agent._add_state_features(next_base, action)
        next_val = agent.actor_critic(torch.FloatTensor(next_state).to(device))[1].item()
        
        # å­˜å‚¨ç»éªŒ
        agent.buffer.append((
            state.copy(),
            action.copy(),
            log_prob,
            value,
            reward,
            next_state.copy(),
            next_val,
            False
        ))
        
        # æ›´æ–°çŠ¶æ€
        base_state = next_base.copy()
        state = next_state.copy()
        
        # PPOæ›´æ–°é€»è¾‘
        if len(agent.buffer) >= BATCH_SIZE:
            states, actions, old_log_probs, values, rewards, next_states, next_vals, dones = zip(*agent.buffer)
            
            # è®¡ç®—GAE
            advantages = []
            returns = []
            for i in range(len(rewards)):
                adv = rewards[i] + GAMMA * next_vals[i] - values[i]
                ret = rewards[i] + GAMMA * next_vals[i]
                advantages.append(adv)
                returns.append(ret)
            
            # è½¬æ¢ä¸ºå¼ é‡
            states_t = torch.FloatTensor(np.array(states)).to(device)
            actions_t = torch.FloatTensor(np.array(actions)).to(device)
            old_log_probs_t = torch.FloatTensor(old_log_probs).to(device)
            returns_t = torch.FloatTensor(returns).to(device)
            advantages_tensor = torch.FloatTensor(advantages).to(device)
            advantages_t = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)
            
            # å¤šè½®æ›´æ–°
            epoch_losses = []
            epoch_value_losses = []
            epoch_entropy = []
            
            for _ in range(PPO_EPOCHS):
                perm = np.random.permutation(len(agent.buffer))
                for start in range(0, len(agent.buffer), MINI_BATCH_SIZE):
                    end = start + MINI_BATCH_SIZE
                    indices = perm[start:end]
                    
                    batch_states = states_t[indices]
                    batch_actions = actions_t[indices]
                    batch_old_log_probs = old_log_probs_t[indices]
                    batch_returns = returns_t[indices]
                    batch_advantages = advantages_t[indices]
                    
                    # å‰å‘ä¼ æ’­
                    action_probs, values = agent.actor_critic(batch_states)
                    dist = Bernoulli(probs=action_probs)
                    new_log_probs = dist.log_prob(batch_actions).sum(dim=1)
                    entropy = dist.entropy().mean()
                    
                    # è®¡ç®—æ¯”ç‡
                    ratio = (new_log_probs - batch_old_log_probs).exp()
                    
                    # ç­–ç•¥æŸå¤±
                    surr1 = ratio * batch_advantages
                    surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * batch_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()
                    
                    # ä»·å€¼æŸå¤±
                    value_loss = F.mse_loss(values, batch_returns)
                    
                    # æ€»æŸå¤±
                    loss = policy_loss + 0.5 * value_loss + ENTROPY_COEF * entropy
                    
                    # åå‘ä¼ æ’­
                    agent.optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(agent.actor_critic.parameters(), 1.0)
                    agent.optimizer.step()
                    
                    # è®°å½•å½“å‰batchçš„æŸå¤±
                    epoch_losses.append(loss.item())
                    epoch_value_losses.append(value_loss.item())
                    epoch_entropy.append(entropy.item())
            
            # è®°å½•å¹³å‡æŸå¤±
            if epoch_losses:
                recent_losses.append(np.mean(epoch_losses))
                recent_value_losses.append(np.mean(epoch_value_losses))
                recent_entropies.append(np.mean(epoch_entropy))
            
            agent.buffer.clear()
        
        # æ„å»ºå½“å‰episodeçš„æ—¥å¿—è®°å½•
        current_log = {
            'episode': episode,
            'reward': reward * REWARD_SCALE,
            'total_loss': recent_losses[-1] if recent_losses else np.nan,
            'value_loss': recent_value_losses[-1] if recent_value_losses else np.nan,
            'entropy': recent_entropies[-1] if recent_entropies else np.nan,
            'num_features': num_features,
            'valid': valid
        }
        episode_logs.append(current_log)
        
        # ä¿å­˜æœ‰æ•ˆç»“æœåˆ°ç¼“å­˜
        if valid:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S%f")
            
            valid_combinations_cache.append({
                'episode': episode,
                'timestamp': timestamp,
                'reward': reward * REWARD_SCALE,
                'num_features': num_features,
                'p_values': p_values,
                'bootstrap_pass_1': bootstrap_passes[0],
                'bootstrap_pass_2': bootstrap_passes[1],
                'features': ','.join(map(str, np.where(action > 0.5)[0])),
            })

        # å®šæœŸè¾“å‡º
        if episode % 100 == 0:
            avg_reward = np.nanmean([log['reward'] for log in episode_logs[-100:]])
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            p_str = [f"{p:.4f}" for p in p_values]
            
            # è®¡ç®—æ¯ç§’å¤„ç†é‡
            elapsed = max(1, time.time() - start_time)
            eps = episode / elapsed
            
            print(f"â³ Episode: {episode:5d} | Eps/s: {eps:.2f} | Avg Reward: {avg_reward:8.1f} | "
                  f"ç‰¹å¾æ•°: {num_features:3.0f} | "
                  f"æœ‰æ•ˆ: {valid!s:5} | på€¼: {p_str} | æ—¶é—´: {current_time}")

        # æ¯500ä»£ä¿å­˜ä¸€æ¬¡
        if episode % 500 == 0 and episode > 0:
            # +++ æ·»åŠ æ¨¡å‹ä¿å­˜åŠŸèƒ½ +++
            torch.save(agent.actor_critic.state_dict(), MODEL_PATH)
            print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹æƒé‡åˆ°: {MODEL_PATH}")
            # +++++++++++++++++++++++
            # ä¿å­˜æœ‰æ•ˆç»„åˆ
            if valid_combinations_cache:
                save_df = pd.DataFrame(valid_combinations_cache)
                header = not os.path.exists(FEATURE_CSV)
                save_df.to_csv(FEATURE_CSV, mode='a', header=header, index=False)
                print(f"ğŸ’¾ ä¿å­˜{len(valid_combinations_cache)}ä¸ªæœ‰æ•ˆç»„åˆï¼ˆEpisode {episode}ï¼‰")
                valid_combinations_cache.clear()
            
            # ä¿å­˜è®­ç»ƒæ—¥å¿—
            log_df = pd.DataFrame(episode_logs)
            log_df.to_csv(os.path.join(OUTPUT_DIR, "training_log.csv"), index=False)
            
            # ç»˜åˆ¶è®­ç»ƒè¿›åº¦å›¾ï¼ˆæ¯500ä»£ä¸€æ¬¡ï¼‰
            plot_path = save_training_progress(episode_logs, OUTPUT_DIR, episode)
            if plot_path:
                print(f"ğŸ“Š è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜: {plot_path}")

    # è®­ç»ƒç»“æŸåä¿å­˜å‰©ä½™ç¼“å­˜
    if valid_combinations_cache:
        save_df = pd.DataFrame(valid_combinations_cache)
        save_df.to_csv(FEATURE_CSV, mode='a', header=False, index=False)
        print(f"ğŸ’¾ ä¿å­˜æœ€å{len(valid_combinations_cache)}ä¸ªæœ‰æ•ˆç»„åˆ")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(OUTPUT_DIR, "final_model.pth")
    torch.save(agent.actor_critic.state_dict(), final_model_path)
    pd.DataFrame(episode_logs).to_csv(os.path.join(OUTPUT_DIR, "training_log.csv"), index=False)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {(time.time()-start_time)/3600:.2f} å°æ—¶")

# ================== BootstrapæŠ½æ ·å‡½æ•° ==================
def bootstrap_sample(dataset):
    """ç”Ÿæˆbootstrapæ ·æœ¬ - æŠ½æ ·é‡ä¸ºåŸæ ·æœ¬çš„0.3å€"""
    time, event, features = dataset
    n = len(time)
    sample_size = max(1, int(n * 0.3))  # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬
    indices = np.random.choice(n, size=sample_size, replace=True)
    return (time[indices], event[indices], features[indices])

if __name__ == "__main__":
    # åŠ è½½æ•°æ®å¹¶è·å–ç‰¹å¾ç»´åº¦
    datasets, feature_dim = load_data(DATA_PATHS)
    
    # é¢„ç”ŸæˆBootstrapæ ·æœ¬
    bootstrap_samples = [
        [bootstrap_sample(datasets[0]) for _ in range(100)],  # è®­ç»ƒç»„è‚ºç™Œ
        [bootstrap_sample(datasets[1]) for _ in range(100)]   # Ntrain
    ]
    
    # å¼€å§‹è®­ç»ƒ
    train(datasets, feature_dim, bootstrap_samples)
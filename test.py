# -*- coding: utf-8 -*-
"""
åŒæ­¥ç‰ˆç‰¹å¾ç»„åˆlogrankæ£€éªŒè®¡ç®—ç¨‹åº
Created on Wed Apr  9 22:44:34 2025
@author: wuli
"""

import numpy as np
import pandas as pd
from lifelines.statistics import logrank_test
import os
import warnings
warnings.filterwarnings('ignore')

# ================== é…ç½®å‚æ•° ==================
DATA_PATHS = {
    "Ntest": "Ntest.xlsx",
    "Ntrain": "Ntrain.xlsx", 
    "Nvalidate": "Nvalidate.xlsx",
    "test": "è®­ç»ƒç»„.xlsx",
    "train": "æµ‹è¯•ç»„.xlsx",
    "validate": "éªŒè¯ç»„æ•´ä½“.xlsx"
}

FEATURE_CSV = "selected_features_train1+2.csv"
OUTPUT_CSV = "all_datasets_pvalues.csv"

# ================== æ•°æ®é¢„åŠ è½½ ==================
def load_all_datasets():
    """ä¸»è¿›ç¨‹ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®"""
    print("ðŸ“š ä¸»è¿›ç¨‹åŠ è½½æ•°æ®...")
    datasets = {}
    for name, path in DATA_PATHS.items():
        df = pd.read_excel(path)
        time_col = df.iloc[:, 0].values.astype(np.float32)
        event_col = df.iloc[:, 1].values
        features = df.iloc[:, 2:].values.astype(np.float32)
        datasets[name] = (time_col, event_col, features)
    return datasets

# ================== på€¼è®¡ç®—å‡½æ•° ==================
def calculate_pvalue_sync(args, datasets):
    """åŒæ­¥è®¡ç®—å•ä¸ªæ•°æ®é›†çš„på€¼"""
    feature_indices, dataset_name = args
    
    try:
        time_data, event_data, features = datasets[dataset_name]
        
        # åˆ›å»ºç‰¹å¾é€‰æ‹©å‘é‡
        action = np.zeros(features.shape[1], dtype=int)
        action[feature_indices] = 1
        
        selected = (features @ action) > 0.5
        group1_samples = np.sum(selected)
        group2_samples = np.sum(~selected)
        
        if group1_samples < 3 or group2_samples < 3:
            return (dataset_name, 1.0, False, group1_samples, group2_samples)
        
        result = logrank_test(time_data[selected], 
                            time_data[~selected],
                            event_observed_A=event_data[selected],
                            event_observed_B=event_data[~selected])
        return (dataset_name, result.p_value, True, group1_samples, group2_samples)
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—é”™è¯¯: {str(e)}")
        return (dataset_name, 1.0, False, 0, 0)

# ================== ä¸»å¤„ç†æµç¨‹ ==================
def main():
    # åŠ è½½æ•°æ®
    all_datasets = load_all_datasets()
    
    # åŠ è½½ç‰¹å¾ç»„åˆ
    print("ðŸ“– è¯»å–ç‰¹å¾ç»„åˆæ–‡ä»¶...")
    features_df = pd.read_csv(FEATURE_CSV)
    features_df['feature_indices'] = features_df['features'].apply(
        lambda x: list(map(int, x.split(','))) if pd.notnull(x) else []
    )
    
    # ç”Ÿæˆä»»åŠ¡å‚æ•°
    print("âš™ï¸ ç”Ÿæˆè®¡ç®—ä»»åŠ¡...")
    task_args = []
    for _, row in features_df.iterrows():
        indices = row['feature_indices']
        for ds_name in DATA_PATHS.keys():
            task_args.append( (indices, ds_name) )
    
    # åŒæ­¥è®¡ç®—
    print("âš¡ å¼€å§‹åŒæ­¥è®¡ç®—...")
    results = []
    
    try:
        from tqdm import tqdm
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        with tqdm(total=len(task_args), desc="å¤„ç†è¿›åº¦") as pbar:
            for args in task_args:
                res = calculate_pvalue_sync(args, all_datasets)
                results.append(res)
                pbar.update()
    except ImportError:
        # æ— tqdmæ—¶çš„å›žé€€æ–¹æ¡ˆ
        for i, args in enumerate(task_args):
            res = calculate_pvalue_sync(args, all_datasets)
            results.append(res)
            if i % 100 == 0:
                print(f"å·²å¤„ç† {i}/{len(task_args)} ä¸ªä»»åŠ¡")
    
    # æž„å»ºç»“æžœå­—å…¸ï¼ˆä¸ŽåŽŸé€»è¾‘ä¸€è‡´ï¼‰
    print("ðŸ“Š æ•´ç†ç»“æžœ...")
    result_map = {}
    for idx, (ds_name, p, valid, n1, n2) in enumerate(results):
        task_idx = idx // len(DATA_PATHS)
        if task_idx not in result_map:
            result_map[task_idx] = {}
        result_map[task_idx][ds_name] = (p, valid, n1, n2)
    
    # æž„å»ºè¾“å‡ºDataFrameï¼ˆä¸ŽåŽŸé€»è¾‘ä¸€è‡´ï¼‰
    output_data = []
    for idx, row in features_df.iterrows():
        record = {
            "episode": row["episode"],
            "features": row["features"],
            "num_features": row["num_features"],
            "original_reward": row["reward"]
        }
        
        if idx in result_map:
            for ds_name in DATA_PATHS.keys():
                p, valid, n1, n2 = result_map[idx].get(ds_name, (1.0, False, 0, 0))
                record.update({
                    f"{ds_name}_p": p,
                    f"{ds_name}_valid": valid,
                    f"{ds_name}_n1": n1,
                    f"{ds_name}_n2": n2
                })
        
        output_data.append(record)
    
    # ä¿å­˜ç»“æžœ
    output_df = pd.DataFrame(output_data)
    columns_order = ["episode", "num_features", "original_reward", "features"] 
    for ds in DATA_PATHS.keys():
        columns_order.extend([f"{ds}_p", f"{ds}_valid", f"{ds}_n1", f"{ds}_n2"])
    
    output_df = output_df[columns_order]
    output_df.to_csv(OUTPUT_CSV, index=False)
    print(f"âœ… ç»“æžœå·²ä¿å­˜è‡³ï¼š{OUTPUT_CSV}")

if __name__ == "__main__":
    main()